import type { ModelDef } from "@koryphaios/shared";

export const GroqModels: ModelDef[] = [
  {
    id: "groq/openai/gpt-oss-120b",
    name: "GPT OSS 120B (Groq)",
    provider: "groq",
    apiModelId: "openai/gpt-oss-120b",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 0.15,
    costPerMOutputTokens: 0.15,
    canReason: true,
    supportsAttachments: false,
    supportsStreaming: true,
    tier: "fast",
  },
  {
    id: "groq/openai/gpt-oss-20b",
    name: "GPT OSS 20B (Groq)",
    provider: "groq",
    apiModelId: "openai/gpt-oss-20b",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 0.05,
    costPerMOutputTokens: 0.05,
    canReason: true,
    supportsAttachments: false,
    supportsStreaming: true,
    tier: "cheap",
  },
  {
    id: "groq/qwen/qwen3-32b",
    name: "Qwen 3 32B (Groq)",
    provider: "groq",
    apiModelId: "qwen/qwen3-32b",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 0.10,
    costPerMOutputTokens: 0.10,
    canReason: false,
    supportsAttachments: false,
    supportsStreaming: true,
    tier: "cheap",
  },
  {
    id: "llama-3.3-70b-versatile",
    name: "Llama 3.3 70B Versatile",
    provider: "groq",
    apiModelId: "llama-3.3-70b-versatile",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    tier: "fast",
  },
  {
    id: "meta-llama/llama-4-scout-17b-16e-instruct",
    name: "Llama 4 Scout",
    provider: "groq",
    apiModelId: "meta-llama/llama-4-scout-17b-16e-instruct",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    tier: "fast",
  },
];
