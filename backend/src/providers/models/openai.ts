import type { ModelDef } from "@koryphaios/shared";

export const OpenAIModels: ModelDef[] = [
  // Current Models (2025-2026)
  {
    id: "gpt-5.2",
    name: "GPT 5.2",
    provider: "openai",
    apiModelId: "gpt-5.2",
    contextWindow: 400_000,
    maxOutputTokens: 128_000,
    costPerMInputTokens: 1.25,
    costPerMOutputTokens: 10.0,
    costPerMInputCached: 0.125,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-5.1",
    name: "GPT 5.1",
    provider: "openai",
    apiModelId: "gpt-5.1",
    contextWindow: 400_000,
    maxOutputTokens: 128_000,
    costPerMInputTokens: 1.25,
    costPerMOutputTokens: 10.0,
    costPerMInputCached: 0.125,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-5",
    name: "GPT 5",
    provider: "openai",
    apiModelId: "gpt-5",
    contextWindow: 400_000,
    maxOutputTokens: 128_000,
    costPerMInputTokens: 1.25,
    costPerMOutputTokens: 10.0,
    costPerMInputCached: 0.125,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-5-mini",
    name: "GPT 5 mini",
    provider: "openai",
    apiModelId: "gpt-5-mini",
    contextWindow: 400_000,
    maxOutputTokens: 64_000,
    costPerMInputTokens: 0.25,
    costPerMOutputTokens: 2.0,
    costPerMInputCached: 0.025,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "fast",
  },
  {
    id: "gpt-5-nano",
    name: "GPT 5 nano",
    provider: "openai",
    apiModelId: "gpt-5-nano",
    contextWindow: 400_000,
    maxOutputTokens: 32_000,
    costPerMInputTokens: 0.05,
    costPerMOutputTokens: 0.40,
    costPerMInputCached: 0.005,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "cheap",
  },
  {
    id: "gpt-5.2-instant",
    name: "GPT-5.2 Instant",
    provider: "openai",
    apiModelId: "gpt-5.2-instant",
    contextWindow: 400_000,
    maxOutputTokens: 64_000,
    costPerMInputTokens: 1.25,
    costPerMOutputTokens: 5.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "fast",
  },

  // Legacy Models (Deprecated from ChatGPT Feb 2026, still in API)
  {
    id: "gpt-4.1",
    name: "GPT 4.1 (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4.1",
    contextWindow: 1_047_576,
    maxOutputTokens: 20_000,
    costPerMInputTokens: 2.0,
    costPerMOutputTokens: 8.0,
    costPerMInputCached: 0.50,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-4.1-mini",
    name: "GPT 4.1 mini (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4.1-mini",
    contextWindow: 200_000,
    maxOutputTokens: 20_000,
    costPerMInputTokens: 0.40,
    costPerMOutputTokens: 1.60,
    costPerMInputCached: 0.10,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "fast",
  },
  {
    id: "gpt-4.1-nano",
    name: "GPT 4.1 nano (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4.1-nano",
    contextWindow: 1_047_576,
    maxOutputTokens: 20_000,
    costPerMInputTokens: 0.10,
    costPerMOutputTokens: 0.40,
    costPerMInputCached: 0.025,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "cheap",
  },
  {
    id: "gpt-4.5-preview",
    name: "GPT 4.5 preview (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4.5-preview",
    contextWindow: 128_000,
    maxOutputTokens: 15_000,
    costPerMInputTokens: 75.0,
    costPerMOutputTokens: 150.0,
    costPerMInputCached: 37.50,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-4o",
    name: "GPT 4o (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4o",
    contextWindow: 128_000,
    maxOutputTokens: 4_096,
    costPerMInputTokens: 2.50,
    costPerMOutputTokens: 10.0,
    costPerMInputCached: 1.25,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "flagship",
  },
  {
    id: "gpt-4o-mini",
    name: "GPT 4o mini (Legacy)",
    provider: "openai",
    apiModelId: "gpt-4o-mini",
    contextWindow: 128_000,
    maxOutputTokens: 4_096,
    costPerMInputTokens: 0.15,
    costPerMOutputTokens: 0.60,
    costPerMInputCached: 0.075,
    costPerMOutputCached: 0.0,
    canReason: false,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "fast",
  },
  {
    id: "o1",
    name: "O1 (Legacy)",
    provider: "openai",
    apiModelId: "o1",
    contextWindow: 200_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 15.0,
    costPerMOutputTokens: 60.0,
    costPerMInputCached: 7.50,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "reasoning",
  },
  {
    id: "o1-pro",
    name: "o1 pro (Legacy)",
    provider: "openai",
    apiModelId: "o1-pro",
    contextWindow: 200_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 150.0,
    costPerMOutputTokens: 600.0,
    costPerMInputCached: 0.0,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "reasoning",
  },
  {
    id: "o1-mini",
    name: "o1 mini (Legacy)",
    provider: "openai",
    apiModelId: "o1-mini",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 1.10,
    costPerMOutputTokens: 4.40,
    costPerMInputCached: 0.55,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "reasoning",
  },
  {
    id: "o3",
    name: "o3 (Legacy)",
    provider: "openai",
    apiModelId: "o3",
    contextWindow: 200_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 10.0,
    costPerMOutputTokens: 40.0,
    costPerMInputCached: 2.50,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "reasoning",
  },
  {
    id: "o3-mini",
    name: "o3 mini (Legacy)",
    provider: "openai",
    apiModelId: "o3-mini",
    contextWindow: 200_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 1.10,
    costPerMOutputTokens: 4.40,
    costPerMInputCached: 0.55,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: false,
    supportsStreaming: true,
    tier: "reasoning",
  },
  {
    id: "o4-mini",
    name: "o4 mini (Legacy)",
    provider: "openai",
    apiModelId: "o4-mini",
    contextWindow: 128_000,
    maxOutputTokens: 50_000,
    costPerMInputTokens: 1.10,
    costPerMOutputTokens: 4.40,
    costPerMInputCached: 0.275,
    costPerMOutputCached: 0.0,
    canReason: true,
    supportsAttachments: true,
    supportsStreaming: true,
    tier: "reasoning",
  },
];
